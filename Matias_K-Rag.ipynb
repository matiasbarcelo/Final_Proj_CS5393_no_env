{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Breaks File into Chunks\n",
    "\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Read the book in smaller chunks to avoid segmentation fault\n",
    "def read_file_in_chunks(file_path, chunk_size=1000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size * 6)  # Read approximately 1000 words (assuming average word length of 6 characters)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "# Process the file in chunks\n",
    "file_path = \"./cleaned_GOT.txt\"\n",
    "chunks = list(read_file_in_chunks(file_path))\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Model\n",
    "Use prebuilt spacy model because building our own model was giving us issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy pasted from instructions\n",
    "\n",
    "def extract_relationships(doc):\n",
    "    relationships = []\n",
    "    for sent in doc.sents:\n",
    "        root = sent.root\n",
    "        subject = None\n",
    "        obj = None\n",
    "        for child in root.children:\n",
    "            if child.dep_ == \"nsubj\":\n",
    "                subject = child\n",
    "            if child.dep_ in [\"dobj\", \"pobj\"]:\n",
    "                obj = child\n",
    "        if subject and obj:\n",
    "            relationships.append((subject, root, obj))\n",
    "    return relationships\n",
    "\n",
    "nlp = spacy.load(\"./domain_ner_model\")\n",
    "\n",
    "def process_document(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    relationships = extract_relationships(doc)\n",
    "    return entities, relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(documents):\n",
    "    G = nx.DiGraph()\n",
    "    for doc in documents:\n",
    "        entities, relationships = process_document(doc)\n",
    "        for entity, entity_type in entities:\n",
    "            G.add_node(entity, type=entity_type)\n",
    "        for subj, pred, obj in relationships:\n",
    "            G.add_edge(subj.text, obj.text, relation=pred.text)\n",
    "    return G\n",
    "\n",
    "documents =   # Your corpus\n",
    "knowledge_graph = build_knowledge_graph(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
