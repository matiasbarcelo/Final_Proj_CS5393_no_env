{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  kRAG - Knowledge Graph-Enhanced RAG System\n",
    "\n",
    "### Objective:\n",
    "Develop a kRAG system that uses Named Entity Recognition (NER) to build a knowledge graph of triples, and then incorporates relevant triples into the prompt for enhanced question answering.\n",
    "\n",
    "##  1: NER and Knowledge Graph Construction\n",
    "\n",
    "### Day 1-3: Data Preparation and NER Model Development\n",
    "\n",
    "1. Choose a domain (e.g., scientific papers, news articles, or technical documentation)\n",
    "2. Collect a corpus of 100-200 documents in the chosen domain\n",
    "3. Implement or fine-tune an NER model using a framework like spaCy or Hugging Face Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model with your domain-specific data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m train_ner_model(\u001b[43mtrain_data\u001b[49m)\n\u001b[0;32m     28\u001b[0m ner_model\u001b[38;5;241m.\u001b[39mto_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./domain_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def train_ner_model(train_data, iterations=30):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, drop=0.5, losses=losses)\n",
    "            print(f\"Iteration {itn}, Losses: {losses}\")\n",
    "\n",
    "    return nlp\n",
    "\n",
    "# Train the model with your domain-specific data\n",
    "ner_model = train_ner_model(train_data)\n",
    "ner_model.to_disk(\"./domain_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Relationship Extraction\n",
    "\n",
    "Implement a relationship extraction module to identify connections between entities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "\n",
    "def extract_relationships(doc):\n",
    "    relationships = []\n",
    "    for sent in doc.sents:\n",
    "        root = sent.root\n",
    "        subject = None\n",
    "        obj = None\n",
    "        for child in root.children:\n",
    "            if child.dep_ == \"nsubj\":\n",
    "                subject = child\n",
    "            if child.dep_ in [\"dobj\", \"pobj\"]:\n",
    "                obj = child\n",
    "        if subject and obj:\n",
    "            relationships.append((subject, root, obj))\n",
    "    return relationships\n",
    "\n",
    "nlp = spacy.load(\"./domain_ner_model\")\n",
    "\n",
    "def process_document(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    relationships = extract_relationships(doc)\n",
    "    return entities, relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Knowledge Graph Construction\n",
    "\n",
    "Build a knowledge graph using the extracted entities and relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_knowledge_graph(documents):\n",
    "    G = nx.DiGraph()\n",
    "    for doc in documents:\n",
    "        entities, relationships = process_document(doc)\n",
    "        for entity, entity_type in entities:\n",
    "            G.add_node(entity, type=entity_type)\n",
    "        for subj, pred, obj in relationships:\n",
    "            G.add_edge(subj.text, obj.text, relation=pred.text)\n",
    "    return G\n",
    "\n",
    "documents = [doc1, doc2, ...]  # Your corpus\n",
    "knowledge_graph = build_knowledge_graph(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
