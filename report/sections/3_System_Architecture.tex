{\color{gray}\hrule}
\begin{center}
\section{System Architecture}
\bigskip
\end{center}
{\color{gray}\hrule}

\subsection{Base Architecture}

\subsubsection{Summary}
The Base Architecture is what is implemented in both RAG and KRAG architectures. The Base Architecture uses the dotenv module to get a user's API key from a .env file, loads up the embeddings with the API key, loads up a vectorstorage variable for similarity search, and loads up large language model (LLM) using the API key. Both RAG and KRAG also have token control functionality as OpenAI models have token limits to control bandwidth.

\subsubsection{Similarity Search}
The similarity search implements the "retrieval" part of "retrieval augment generate". The system retrieves OpenAI's embeddings and Facebook AI Similarity Search (FAISS) using the langchain module. The system passes the processed chunks of \textit{A Song of Ice and Fire} books and OpenAI's embeddings into the FAISS object and can retrieve the k most similar chunks of given parameters query and integer k using its similarity\_search method.

\subsubsection{Token Control}
OpenAI's gpt-3.5-turbo model has an total token limit of 4096 tokens and reserves 256 tokens for generating a response. This gives the system 3840 tokens for a query. Both RAG and KRAG implementations use the tiktoken module to determine what tokens will go in the query.

\subsection{RAG}
The RAG architecture passes 2 things to the query: a context and a question. First, a user enters a question to the system. As mentioned earlier, the "retrieval" in "retrieval augment generate" is done using FAISS. FAISS finds the k-nearest chunks or files of a given question, which in this implementation is the three nearest 1000 character chunks of the books; what is retrieved by this similarity search is the "context" of the query. Since a user's question cannot be cut from a query, the system does a calculation to see how many tokens need to be cut from the context in order to meet the token requirements. Once the context tokens are cut appropriately, the context and question are passed to a template that serve as the query passed to the LLM for a response. The context and question stitched to a template for query is the "augment" portion of "retrieval augment generate" and the response from the LLM is the "generate" portion.

\subsection{KRAG}
The KRAG implementation is nearly identical to the RAG implementation, but it includes an addition query item, knowledge graphs triples, and token control for the additional item. Given a question by a user, the KRAG query, on top of what the RAG query already does, finds the entities in the given question that match those in the knowledge graph, finds the relationships that entity has with other entities/nodes, and returns that information in the form of triples, a data structure developed by Tim Berners-Lee as mentioned in class. After some token control, those triples are passed to the query along with everything else, which is passed to the LLM to generate a response.

\bigskip