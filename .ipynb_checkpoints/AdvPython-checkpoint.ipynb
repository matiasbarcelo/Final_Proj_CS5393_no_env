{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'book_data.ttl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     22\u001b[0m book_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a book. It contains information. Each sentence is important.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m save_book_to_ttl(book_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbook_data.ttl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36msave_book_to_ttl\u001b[0;34m(book_text, ttl_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m     g\u001b[38;5;241m.\u001b[39madd((subject, predicate, obj))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Save to Turtle format\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ttl_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(g\u001b[38;5;241m.\u001b[39mserialize(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mttl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'book_data.ttl'"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "\n",
    "# Define RDF namespace\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "\n",
    "def save_book_to_ttl(book_text, ttl_file):\n",
    "    g = Graph()\n",
    "    g.bind(\"ex\", EX)\n",
    "    \n",
    "    sentences = book_text.split(\". \")  # Split text into chunks\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        subject = URIRef(f\"http://example.org/sentence/{i}\")\n",
    "        predicate = EX[\"content\"]\n",
    "        obj = Literal(sentence)\n",
    "        g.add((subject, predicate, obj))\n",
    "    \n",
    "    # Save to Turtle format\n",
    "    with open(ttl_file, \"wb\") as f:\n",
    "        f.write(g.serialize(format=\"ttl\"))\n",
    "\n",
    "# Example usage\n",
    "book_text = \"This is a book. It contains information. Each sentence is important.\"\n",
    "save_book_to_ttl(book_text, \"book_data.ttl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ttl(ttl_file, search_term):\n",
    "    g = Graph()\n",
    "    g.parse(ttl_file, format=\"ttl\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT ?sentence\n",
    "    WHERE {{\n",
    "        ?s <http://example.org/content> ?sentence .\n",
    "        FILTER(CONTAINS(LCASE(STR(?sentence)), LCASE(\"{search_term}\")))\n",
    "    }}\n",
    "    \"\"\"\n",
    "    results = g.query(query)\n",
    "    return [str(row[0]) for row in results]\n",
    "\n",
    "# Example usage\n",
    "search_results = query_ttl(\"book_data.ttl\", \"information\")\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load Llama2 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\")\n",
    "\n",
    "def ask_llm(model, tokenizer, query, context):\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the book about?\"\n",
    "context = \" \".join(search_results)  # Combine retrieved results\n",
    "response = ask_llm(model, tokenizer, query, context)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(book_file, ttl_file, query):\n",
    "    # Step 1: Save book to .ttl\n",
    "    with open(book_file, \"r\") as f:\n",
    "        book_text = f.read()\n",
    "    save_book_to_ttl(book_text, ttl_file)\n",
    "    \n",
    "    # Step 2: Query the `.ttl` file\n",
    "    context = \" \".join(query_ttl(ttl_file, query))\n",
    "    \n",
    "    # Step 3: Generate response using LLM\n",
    "    response = ask_llm(model, tokenizer, query, context)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "print(rag_pipeline(\"book.txt\", \"book_data.ttl\", \"information\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
